{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Deep Learning Model in PyTorch and Export to ONNX\n",
    "\n",
    "This tutorial will train a CNN in PyTorch and covert to ONNX. Once the model is in ONNX format, we can import that into other frameworks such as TF for inference or reuse the model through transfer learning\n",
    "\n",
    "Reference:\n",
    "https://thenewstack.io/tutorial-train-a-deep-learning-model-in-pytorch-and-export-it-to-onnx/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to define NN with appropriate layers\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create method to train PyTorch model\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    " \n",
    "    test_loss /= len(test_loader.dataset)\n",
    " \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train CNN with MNIST Dataset\n",
    "\n",
    "**Workflow**\n",
    "1. Download MNIST train/test sets from torch utils\n",
    "2. Preprocess by normalizing mean and std deviation.\n",
    "3. Define optimizer\n",
    "4. Train model using up to 10 epochs\n",
    "5. Save PyTorch model within working directory\n",
    "6. Print model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\desktop\\code_717\\miniconda3\\envs\\mi2_pyt2tf\\lib\\site-packages\\torch\\autograd\\__init__.py:132: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.312158\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 0.540955\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 0.297920\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.276051\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.189621\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.094165\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.098553\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.082709\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.100938\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.198560\n",
      "\n",
      "Test set: Average loss: 0.1020, Accuracy: 9681/10000 (97%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.068579\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.139775\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.069008\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.104723\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.075920\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.075243\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.071490\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.022305\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.017888\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.119180\n",
      "\n",
      "Test set: Average loss: 0.0606, Accuracy: 9801/10000 (98%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.120160\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.094332\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.018692\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.033517\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.055741\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.019394\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.041908\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.274838\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.035891\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.023634\n",
      "\n",
      "Test set: Average loss: 0.0455, Accuracy: 9851/10000 (99%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.052370\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.235848\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.024919\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.060027\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.020411\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.066622\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.028723\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.028502\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.049907\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.044037\n",
      "\n",
      "Test set: Average loss: 0.0385, Accuracy: 9867/10000 (99%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.008240\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.024149\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.015267\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.066653\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.028193\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.079273\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.083103\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.078206\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.048247\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.032248\n",
      "\n",
      "Test set: Average loss: 0.0373, Accuracy: 9881/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download MNIST dataset, preprocess, and train \n",
    "device =  \"cpu\"\n",
    "modelName = \"mnist-pyt.pt\"\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "               transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "               ])),\n",
    "                batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "               transform=transforms.Compose([\n",
    "                   transforms.ToTensor(),\n",
    "                   transforms.Normalize((0.1307,), (0.3081,))\n",
    "               ])),\n",
    "                batch_size=1000, shuffle=True)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "for epoch in range(0, 5):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Model Saved:  mnist-pyt.pt\n"
     ]
    }
   ],
   "source": [
    "# Save PyTorch model\n",
    "torch.save(model.state_dict(), modelName)\n",
    "\n",
    "print(\"PyTorch Model Saved: \", modelName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Summary\n",
    "\n",
    "There are a couple ways of producing a Keras-like model summary by using the torchvision and torchsummary packages. Both require {model, input_size}\n",
    "\n",
    "The input size determines the size of the \n",
    "\n",
    "Reference: \n",
    "http://jkimmel.net/pytorch_estimating_model_size/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 20, 24, 24]             520\n",
      "            Conv2d-2             [-1, 50, 8, 8]          25,050\n",
      "            Linear-3                  [-1, 500]         400,500\n",
      "            Linear-4                   [-1, 10]           5,010\n",
      "================================================================\n",
      "Total params: 431,080\n",
      "Trainable params: 431,080\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.12\n",
      "Params size (MB): 1.64\n",
      "Estimated Total Size (MB): 1.76\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print Model Summary\n",
    "from torchvision import models\n",
    "from torchsummary import summary\n",
    "\n",
    "summaryChoice = \"default\" # {default | vgg}\n",
    "\n",
    "if summaryChoice == \"default\": \n",
    "    # Get shape and print model\n",
    "    summary(model, (1, 28, 28))\n",
    "else:    \n",
    "    # Get shape using VGG16\n",
    "    vgg = models.vgg16()\n",
    "    summary(vgg, (3, 224, 224)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting PyTorch to ONNX\n",
    "\n",
    "PyTorch support ONNX natively so we can convert model without an additional module.\n",
    "\n",
    "**Workflow**\n",
    "1. Load trained PyTorch model\n",
    "2. Create input that matches shape of input tensor\n",
    "3. Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch model loaded:  mnist-pyt.pt\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "trained_model = Net()\n",
    "trained_model.load_state_dict(torch.load(modelName))\n",
    "print(\"PyTorch model loaded: \", modelName)\n",
    "dummy_input = Variable(torch.randn(1, 1, 28, 28)) \n",
    "\n",
    "modelName = \"mnist-pyt.onnx\"\n",
    "torch.onnx.export(trained_model, dummy_input, modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'mnistpyt.onnx' at http://localhost:8081\n"
     ]
    }
   ],
   "source": [
    "# View model using Netron (if installed)\n",
    "import netron\n",
    "\n",
    "netron.start(modelName, port=8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
