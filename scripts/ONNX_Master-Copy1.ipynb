{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Models via ONNX \n",
    "This script is designed to be the end-to-end master for converting models from one framework to another. The following frameworks are included:\n",
    "* Keras\n",
    "* Matlab (future)\n",
    "* PyTorch\n",
    "* Scikit-Learn\n",
    "* TensorFlow\n",
    "\n",
    "This scipt is to act as a function with the following parameters:\n",
    "**Input(s):**\n",
    "1. Model file name (provide absolute path if not within current dir)\n",
    "2. 'frameworkIn' format\n",
    "3. 'frameworkOut' format\n",
    "\n",
    "**Output(s):**\n",
    "1. Model in ONNX format\n",
    "2. Model in 'frameworkOut' format\n",
    "\n",
    "**Future Work:**\n",
    "* Create folders for *modelONNX* and *modelOut* files\n",
    "* Validate front and backend work with onnx==1.6.0 and onnx_tf==1.6.0\n",
    "* Display completion time of entire convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User Inputs:\n",
    "frameworkIn = \"onnx\"    # {keras | matlab | onnx | pytorch | scikit-learn | tensorflow}\n",
    "frameworkOut = \"keras\"\n",
    "\n",
    "#pathIn = \"cifar_entire.pth\"\n",
    "onnxPath = 'output_ONNX/'\n",
    "outPath = 'output/'\n",
    "\n",
    "netronView = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- 'ONNX_Master.ipynb' ---- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Base Imports\n",
    "import netron\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# ONNX\n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "# Tensorflow\n",
    "#import tensorflow as tf\n",
    "\n",
    "print(\"---- 'ONNX_Master.ipynb' ---- \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Version:\n",
      "numpy:\t\t 1.19.2 \n",
      "tensorflow:\t 2.2.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Local py scripts:\n",
    "import convertToONNX\n",
    "import convertFromONNX\n",
    "import util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package Versions \n",
      "onnx:\t\t 1.6.0 \n",
      "onnxruntime:\t 1.5.2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Package Versions\", \"\\n\"\n",
    "      #\"numpy:\\t\\t\", numpy.__version__, '\\n'\n",
    "      \"onnx:\\t\\t\", onnx.__version__, '\\n'\n",
    "      \"onnxruntime:\\t\", onnxruntime.__version__, '\\n'\n",
    "      #\"skl2onnx:\\t\", skl2onnx.__version__, \"\\n\"\n",
    "      #\"tensorflow:\\t\", tf.__version__, '\\n'\n",
    "      #\"torch:\\t\\t\", torch.__version__, \"\\n\"\n",
    "      #\"torchvision:\\t\", torchvision.__version__, \"\\n\"\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug helper - assumes all models are local \n",
    "if frameworkIn == \"keras\":\n",
    "    pathIn = \"mnist-model.h5\"\n",
    "elif frameworkIn == \"onnx\":\n",
    "    #pathIn = \"mnist_tf.onnx\"\n",
    "    pathIn = \"vgg16.onnx\"\n",
    "elif frameworkIn == \"pytorch\":\n",
    "    pathIn = \"vgg16.pth\"\n",
    "elif frameworkIn == \"scikit-learn\":\n",
    "    pathIn = \"iris-model.pkl\"\n",
    "elif frameworkIn == \"tensorflow\":\n",
    "    pathIn = \"MNIST_tf.pb\"              # Not working\n",
    "    pathIn = \"mnist_tf.h5\"\n",
    "else:\n",
    "    print(\"ERROR: Invalid framework!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input File Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path:  vgg16.onnx \n",
      "\n",
      "Model imported:\t\t vgg16.onnx \n",
      "Model converted:\t output_ONNX/vgg16.onnx\n"
     ]
    }
   ],
   "source": [
    "# Get import file\n",
    "pathIn = pathlib.Path(pathIn)\n",
    "modelIn = pathIn.name\n",
    "modelONNX, baseName = util.ChangeExtension(modelIn, 'onnx') \n",
    "\n",
    "# Check for output directories provided by user\n",
    "util.CheckDirectories(onnxPath, outPath)\n",
    "modelONNX = onnxPath + modelONNX\n",
    "\n",
    "print(\"File path: \", pathIn, \"\\n\\n\"\n",
    "      \"Model imported:\\t\\t\", modelIn, \"\\n\"\n",
    "      \"Model converted:\\t\", modelONNX)\n",
    "\n",
    "# Make sure modelIn is located within current working directory\n",
    "util.CopyToDirectory(pathIn, modelIn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting model:  onnx  to ONNX...\n",
      "\n",
      "Model already in ONNX format!\n",
      "Conversion complete!\n",
      " -----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run conversion script\n",
    "print(\"Converting model: \", frameworkIn, \" to ONNX...\\n\")\n",
    "    \n",
    "# Import framework specific packages  \n",
    "if frameworkIn == \"keras\":\n",
    "    convertToONNX.Keras_ONNX(modelIn, modelONNX)           # Tested\n",
    "elif frameworkIn == \"pytorch\":\n",
    "    convertToONNX.PyTorch_ONNX(modelIn, modelONNX)         # Tested \n",
    "elif frameworkIn == \"onnx\":\n",
    "    print(\"Model already in ONNX format!\")\n",
    "elif frameworkIn == \"scikit-learn\":\n",
    "    convertToONNX.ScikitLearn_ONNX(modelIn, modelONNX)     # Tested\n",
    "elif frameworkIn == \"tensorflow\":\n",
    "    convertToONNX.TensorFlow_ONNX(modelIn, modelONNX)      # Tested (h5)\n",
    "else:\n",
    "    print(\"Invalid framework chosen.\")\n",
    "\n",
    "print(\"Conversion complete!\\n\", \"-\"*65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Verify ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output_ONNX/vgg16.onnx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a2a6b9707986>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load ONNX Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0monnxIn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0monnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelONNX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Check that IR is well formed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0monnx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchecker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monnxIn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Desktop\\Code_717\\miniconda3\\envs\\onnx_master_2\\lib\\site-packages\\onnx\\__init__.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(f, format, load_external_data)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mLoaded\u001b[0m \u001b[1;32min\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mmemory\u001b[0m \u001b[0mModelProto\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     '''\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model_from_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Desktop\\Code_717\\miniconda3\\envs\\onnx_master_2\\lib\\site-packages\\onnx\\__init__.py\u001b[0m in \u001b[0;36m_load_bytes\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mreadable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output_ONNX/vgg16.onnx'"
     ]
    }
   ],
   "source": [
    "# Load ONNX Model\n",
    "onnxIn = onnx.load(modelONNX)\n",
    "\n",
    "# Check that IR is well formed\n",
    "onnx.checker.check_model(onnxIn)\n",
    "print(modelONNX, \" has been loaded and checked!\")\n",
    "\n",
    "# Run inference using ONNX Runtime\n",
    "convertToONNX.ONNX_Inference(modelONNX)\n",
    "\n",
    "# Use Netron to view and verify model\n",
    "if netronView:\n",
    "    print(\"Loading Netron in separate tab...\")\n",
    "    netron.start(modelONNX, port=8081)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to FrameworkOut\n",
    "\n",
    "**NOTE: As of 12/8/2020 there are issues with onnx_tf==1.7.0 causing the Jupyter Notebook kernel to crash. The following is needed:**\n",
    "* onnx     1.6.0\n",
    "* onnx_tf  1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting Class (ONNX -> frameworkOut)\n",
    "if frameworkOut == \"keras\": \n",
    "    modelOut, _ = util.ChangeExtension(modelONNX, \"h5\")       # \n",
    "    convertFromONNX.ONNX_Keras(onnxIn, modelOut)\n",
    "elif frameworkOut == \"onnx\":                     \n",
    "    print(\"ONNX file already created!\")\n",
    "elif frameworkOut == \"pytorch\": \n",
    "    modelOut, _ = util.ChangeExtension(modelONNX, \"pt\")       #\n",
    "    convertFromONNX.ONNX_PyTorch(onnxIn, modelOut)\n",
    "elif frameworkOut == \"scikit-learn\": \n",
    "    modelOut, _ = util.ChangeExtension(modelONNX, \"pb\")       #\n",
    "    convertFromONNX.ONNX_ScikitLearn(onnxIn, modelOut), \n",
    "elif frameworkOut == \"tensorflow\": \n",
    "    modelOut, _ = util.ChangeExtension(modelONNX, \"tf\")       # Tested (doesn't work on ALL models)\n",
    "    \n",
    "    print(\"WARNING: There are currently bugs in onnx-tf 1.6.0 and 1.7.0\")\n",
    "    convertFromONNX.ONNX_TensorFlow(onnxIn, modelOut),\n",
    "    \n",
    "print(\"Conversion complete!\")\n",
    "print(\"-\"*65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
